{
    "version": 3,
    "terraform_version": "0.11.7",
    "serial": 4,
    "lineage": "58786f19-0329-f97a-d8bf-f844a5fc8ef1",
    "modules": [
        {
            "path": [
                "root"
            ],
            "outputs": {},
            "resources": {
                "data.null_data_source.values_data": {
                    "type": "null_data_source",
                    "depends_on": [
                        "local.app_name",
                        "local.costcode",
                        "local.environment_types",
                        "local.environments",
                        "local.organisation",
                        "local.scaling_enabled"
                    ],
                    "primary": {
                        "id": "static",
                        "attributes": {
                            "has_computed_default": "default",
                            "id": "static",
                            "inputs.%": "6",
                            "inputs.APP_NAME": "sql_exporter",
                            "inputs.COSTCODE": "cotd",
                            "inputs.ENV_NAME": "ops",
                            "inputs.ENV_TYPE": "production",
                            "inputs.IMAGE": "cgws/sql_exporter",
                            "inputs.SCALING_ENABLED": "false",
                            "outputs.%": "6",
                            "outputs.APP_NAME": "sql_exporter",
                            "outputs.COSTCODE": "cotd",
                            "outputs.ENV_NAME": "ops",
                            "outputs.ENV_TYPE": "production",
                            "outputs.IMAGE": "cgws/sql_exporter",
                            "outputs.SCALING_ENABLED": "false",
                            "random": "1791974112910379197"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "data.template_file.values": {
                    "type": "template_file",
                    "depends_on": [
                        "data.null_data_source.values_data.*",
                        "local.app_name",
                        "local.environments"
                    ],
                    "primary": {
                        "id": "1d48cd6d0e7e456db55bda64f24d535604b2f4fe69bc17fba8f629fcda952815",
                        "attributes": {
                            "id": "1d48cd6d0e7e456db55bda64f24d535604b2f4fe69bc17fba8f629fcda952815",
                            "rendered": "---\ncostcode: cotd\n\nconfig:\n  # See veritas flux release for main config\n  \n  # Global settings and defaults.\n  global:\n    # Scrape timeouts ensure that:\n    #   (i)  scraping completes in reasonable time and\n    #   (ii) slow queries are canceled early when the database is already under heavy load\n    # Prometheus informs targets of its own scrape timeout (via the \"X-Prometheus-Scrape-Timeout-Seconds\" request header)\n    # so the actual timeout is computed as:\n    #   min(scrape_timeout, X-Prometheus-Scrape-Timeout-Seconds - scrape_timeout_offset)\n    #\n    # If scrape_timeout \u003c= 0, no timeout is set unless Prometheus provides one. The default is 10s.\n    scrape_timeout: 5s\n    # Subtracted from Prometheus' scrape_timeout to give us some headroom and prevent Prometheus from timing out first.\n    #\n    # Must be strictly positive. The default is 500ms.\n    scrape_timeout_offset: 500ms\n    # Minimum interval between collector runs: by default (0s) collectors are executed on every scrape.\n    min_interval: 10s\n    # Maximum number of open connections to any one target. Metric queries will run concurrently on multiple connections,\n    # as will concurrent scrapes.\n    #\n    # If max_connections \u003c= 0, then there is no limit on the number of open connections. The default is 3.\n    max_connections: 10\n    # Maximum number of idle connections to any one target. Unless you use very long collection intervals, this should\n    # always be the same as max_connections.\n    #\n    # If max_idle_connections \u003c= 0, no idle connections are retained. The default is 3.\n    max_idle_connections: 10\n\n## Details about the image to be pulled.\nimage:\n  name: 362995399210.dkr.ecr.ap-southeast-2.amazonaws.com/cgws/sql_exporter\n  tag: latest\n  # Try to leave as IfNotPresent, but Always can be used to force image updates at the cost of slower boot times\n  pullPolicy: IfNotPresent\n\n# priorityClassName: \"\"  # Not working yet, allows defining relative pod priority\n\n## Number of instances of the same service to run (how many copies of the pod to run)\nreplicas: 1\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxUnavailable: 1\n\nscaling:\n  enabled: 0\n  min: 1\n  max: 4\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        targetAverageUtilization: 90\n# awsenv required config for envvar loading\nssm:\n  awsRegion: ap-southeast-2\n  path: /sql_exporter/production/ops\n\n# vaultenv required config for envvar loading\n# vault:\n#   secret: /app/appname/environment\n#   aws_auth_mount_path: aws-rbt\n#   role: kubernetes\n\n## Arbitrary Annotations to be added to pods\n## Good for kube2iam, etc\npodAnnotations:\n  log.config.scalyr.com/attributes.parser: loggerNode\n\npodMetrics:\n  port: 9399\n  path: /metrics\n\n## Add any additional pod labels\npodLabels: {}\n\n## Add extra arguments to the container execution command\nextraArgs: {}\n#  key: value   # provided to app as --key=value\n#  key          # provided to app as --key\n\n## Environment variables for the container\n## The default chart deployment is for a single container and will require\n## modification for multiples.\n\nextraEnv: []\n\npodVolumes: false\n\n## These two health check probes should be used to ensure desired behaviour\n## during app failure, maintenance, or other issues. They should point at\n## endpoints that produce different results under different circumstances,\n## if possible.\n## Liveness probes destroy and replace the pod on failure\n# livenessProbe:\n#   httpGet:\n#     path: /healthz\n#     port: 9910\n#     initialDelaySeconds: 3  # Pod boot time\n#     scheme: HTTP\n## Readiness probes leave the pod running but detach from load balancers\n## during failure (i.e. the pods stop receiving requests)\n# readinessProbe:\n#   httpGet:\n#     path: /healthz?isDbUp\u0026isRedisUp\u0026amIConfigured\n#     port: 9910\n#     initialDelaySeconds: 3\n#     scheme: HTTPS\n\n## Every service requires a sane limit as kube doesn't manage resources well enough to protect its core functions\n## Ensure the limit is large enough not to get in the way - it should only be hit\n## if something has gone terribly wrong as it will result in the death of the pod \nresources:\n  limits:\n    memory: 2048Mi\n    cpu: 2000m\n  # Request about what the app will need on average, but low ball it\n  # Requests are **guarantees** and are bad for utilisation optimisation\n  requests:\n    memory: 50Mi\n    cpu: 10m\n\n## rbac should only be necessary for accessing the cluster.\n## Come see ops if you need it.\n## This is here mostly as a placeholder.\nrbac:\n  ## If true, create \u0026 use RBAC resources\n  ##\n  create: false\n  # Beginning with Kubernetes 1.8, the api is stable and v1 can be used.\n  apiVersion: v1\n\n  ## Ignored if rbac.create is true\n  ##\n  serviceAccountName: default\n\ningress:\n  enabled: false\n  labels: {}\n  # Used to create Ingress record (should used with service.type: ClusterIP).\n  # hosts:\n  #  - service.cgws.com.au\n  annotations: {}\n  ## External-DNS will create and manage the dns record for you\n  #   external-dns.alpha.kubernetes.io/hostname: \"service.cgws.com.au\"\n  #   external-dns.alpha.kubernetes.io/ttl: \"60\"\n  ## For apps that don't need dedicated load balancers (not many should), utilise the\n  ## existing cluster nginx ingresses: nginx|nginx-dmz|nginx-public\n  ## Depending on the cluster, nginx-public may not exist - talk to ops if you need it.\n  #   kubernetes.io/ingress.class: nginx-dmz\n  ## For apps that require a dedicated load balancer, an ALB can be configured as below.\n  ## Note that where these example values are regex they will not work without modification.\n  ## See https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/86ceee1e0ff05b0f414bf831f131a4ab95bcf923/docs/ingress-resources.md\n  # kubernetes.io/ingress.class: alb\n  # ## Create an external vs internal lb\n  # alb.ingress.kubernetes.io/scheme: /internet-facing|internal/\n  # ## Which security groups to attach. Omission will result in a cluster-managed, publicly-accessible SG being created and attached\n  # alb.ingress.kubernetes.io/security-groups: sg-723a380a,sg-a6181ede,sg-a5181edd\n  # ## Certificate to attach to https listener\n  # alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:511282955421:certificate/04793e46-aea2-4af8-8165-7e66827d12f2\n  # ## Always include at least these tags\n  # alb.ingress.kubernetes.io/tags: Environment={{ .Values.environment }},costcode={{ .Values.costcode }}\n  # ## Logs to S3, always include\n  # alb.ingress.kubernetes.io/attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=cgws-elb-logs\n  # ## Ports to create listeners on\n  # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":80,\"HTTPS\":443}]'\n  # ## Healthcheck config\n  # alb.ingress.kubernetes.io/healthcheck-interval-seconds: \"6\"\n  # alb.ingress.kubernetes.io/healthcheck-path: /\n  # alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n  # alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \"5\"\n  # alb.ingress.kubernetes.io/healthy-threshold-count: \"4\"\n  # alb.ingress.kubernetes.io/unhealthy-threshold-count: \"2\"\n  # alb.ingress.kubernetes.io/success-codes: \"200\"\n  # ## Deregistration is frustratingly long (5 or 10 minutes) by default, so set it to something less change prohibitive\n  # alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=60\n  # ## Create lb as ipv6+ipv4 or just ipv4\n  # alb.ingress.kubernetes.io/ip-address-type: /dualstack|ipv4/\n  # ## TLS security policy to use on HTTPS listeners. Try not to use anything older than ELBSecurityPolicy-TLS-1-2-2017-01\n  # alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-2017-01\n  # ## Redirects HTTP to HTTPS natively (within the LB)\n  # ## See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/tasks/ssl_redirect/\n  # ## NOTE: This has not been tested since the original implementation which was thoroughly broken\n  # alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\": {\"Protocol\": \"HTTPS\", \"StatusCode\": \"HTTP_301\"}}'\n\nservice:\n  ## Allows arbitrary service annotations like those for AWS load balancers to be added\n  ## See https://kubernetes.io/docs/concepts/services-networking/ for aws LoadBalancer annotations\n  annotations: {}\n    # external-dns.alpha.kubernetes.io/hostname: \"service.env.cgws.com.au\"\n    # external-dns.alpha.kubernetes.io/ttl: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:ap-southeast-2:\u003caccount-id\u003e:certificate/\u003ccert-id\u003e\"\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n    # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n    # service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \"environment=env,costcode=costcode\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \"\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \"3\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \"20\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \"5\"\n    # service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: \"sg-53fae93f,sg-42efd82e\"\n\n  servicePort: 9399  # Main port from which the app serves\n\n  type: ClusterIP  # (ClusterIP|LoadBalancer) Should almost always be ClusterIP\n  \n  clusterIP: \"\"  # Probably leave blank\n  loadBalancerIP: \"\"  # Set ONLY if the app must have its own LB, for general external access use ingress\n  loadBalancerSourceRanges: []  # Security group inbound source IP ranges with allowed access\n\n## nodeSelectors, tolerations, and affinity allow running the pods in specific places,\n## like on master nodes instead of workers.\n## Try not to use these.\n\n## Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n\n## List of node taints to tolerate (requires Kubernetes \u003e= 1.6)\ntolerations: []\n\n## Affinity\naffinity: []\n",
                            "template": "---\ncostcode: cotd\n\nconfig:\n  # See veritas flux release for main config\n  \n  # Global settings and defaults.\n  global:\n    # Scrape timeouts ensure that:\n    #   (i)  scraping completes in reasonable time and\n    #   (ii) slow queries are canceled early when the database is already under heavy load\n    # Prometheus informs targets of its own scrape timeout (via the \"X-Prometheus-Scrape-Timeout-Seconds\" request header)\n    # so the actual timeout is computed as:\n    #   min(scrape_timeout, X-Prometheus-Scrape-Timeout-Seconds - scrape_timeout_offset)\n    #\n    # If scrape_timeout \u003c= 0, no timeout is set unless Prometheus provides one. The default is 10s.\n    scrape_timeout: 5s\n    # Subtracted from Prometheus' scrape_timeout to give us some headroom and prevent Prometheus from timing out first.\n    #\n    # Must be strictly positive. The default is 500ms.\n    scrape_timeout_offset: 500ms\n    # Minimum interval between collector runs: by default (0s) collectors are executed on every scrape.\n    min_interval: 10s\n    # Maximum number of open connections to any one target. Metric queries will run concurrently on multiple connections,\n    # as will concurrent scrapes.\n    #\n    # If max_connections \u003c= 0, then there is no limit on the number of open connections. The default is 3.\n    max_connections: 10\n    # Maximum number of idle connections to any one target. Unless you use very long collection intervals, this should\n    # always be the same as max_connections.\n    #\n    # If max_idle_connections \u003c= 0, no idle connections are retained. The default is 3.\n    max_idle_connections: 10\n\n## Details about the image to be pulled.\nimage:\n  name: 362995399210.dkr.ecr.ap-southeast-2.amazonaws.com/${IMAGE}\n  tag: latest\n  # Try to leave as IfNotPresent, but Always can be used to force image updates at the cost of slower boot times\n  pullPolicy: IfNotPresent\n\n# priorityClassName: \"\"  # Not working yet, allows defining relative pod priority\n\n## Number of instances of the same service to run (how many copies of the pod to run)\nreplicas: 1\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxUnavailable: 1\n\nscaling:\n  enabled: ${SCALING_ENABLED}\n  min: 1\n  max: 4\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        targetAverageUtilization: 90\n# awsenv required config for envvar loading\nssm:\n  awsRegion: ap-southeast-2\n  path: /${APP_NAME}/${ENV_TYPE}/${ENV_NAME}\n\n# vaultenv required config for envvar loading\n# vault:\n#   secret: /app/appname/environment\n#   aws_auth_mount_path: aws-rbt\n#   role: kubernetes\n\n## Arbitrary Annotations to be added to pods\n## Good for kube2iam, etc\npodAnnotations:\n  log.config.scalyr.com/attributes.parser: loggerNode\n\npodMetrics:\n  port: 9399\n  path: /metrics\n\n## Add any additional pod labels\npodLabels: {}\n\n## Add extra arguments to the container execution command\nextraArgs: {}\n#  key: value   # provided to app as --key=value\n#  key          # provided to app as --key\n\n## Environment variables for the container\n## The default chart deployment is for a single container and will require\n## modification for multiples.\n\nextraEnv: []\n\npodVolumes: false\n\n## These two health check probes should be used to ensure desired behaviour\n## during app failure, maintenance, or other issues. They should point at\n## endpoints that produce different results under different circumstances,\n## if possible.\n## Liveness probes destroy and replace the pod on failure\n# livenessProbe:\n#   httpGet:\n#     path: /healthz\n#     port: 9910\n#     initialDelaySeconds: 3  # Pod boot time\n#     scheme: HTTP\n## Readiness probes leave the pod running but detach from load balancers\n## during failure (i.e. the pods stop receiving requests)\n# readinessProbe:\n#   httpGet:\n#     path: /healthz?isDbUp\u0026isRedisUp\u0026amIConfigured\n#     port: 9910\n#     initialDelaySeconds: 3\n#     scheme: HTTPS\n\n## Every service requires a sane limit as kube doesn't manage resources well enough to protect its core functions\n## Ensure the limit is large enough not to get in the way - it should only be hit\n## if something has gone terribly wrong as it will result in the death of the pod \nresources:\n  limits:\n    memory: 2048Mi\n    cpu: 2000m\n  # Request about what the app will need on average, but low ball it\n  # Requests are **guarantees** and are bad for utilisation optimisation\n  requests:\n    memory: 50Mi\n    cpu: 10m\n\n## rbac should only be necessary for accessing the cluster.\n## Come see ops if you need it.\n## This is here mostly as a placeholder.\nrbac:\n  ## If true, create \u0026 use RBAC resources\n  ##\n  create: false\n  # Beginning with Kubernetes 1.8, the api is stable and v1 can be used.\n  apiVersion: v1\n\n  ## Ignored if rbac.create is true\n  ##\n  serviceAccountName: default\n\ningress:\n  enabled: false\n  labels: {}\n  # Used to create Ingress record (should used with service.type: ClusterIP).\n  # hosts:\n  #  - service.cgws.com.au\n  annotations: {}\n  ## External-DNS will create and manage the dns record for you\n  #   external-dns.alpha.kubernetes.io/hostname: \"service.cgws.com.au\"\n  #   external-dns.alpha.kubernetes.io/ttl: \"60\"\n  ## For apps that don't need dedicated load balancers (not many should), utilise the\n  ## existing cluster nginx ingresses: nginx|nginx-dmz|nginx-public\n  ## Depending on the cluster, nginx-public may not exist - talk to ops if you need it.\n  #   kubernetes.io/ingress.class: nginx-dmz\n  ## For apps that require a dedicated load balancer, an ALB can be configured as below.\n  ## Note that where these example values are regex they will not work without modification.\n  ## See https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/86ceee1e0ff05b0f414bf831f131a4ab95bcf923/docs/ingress-resources.md\n  # kubernetes.io/ingress.class: alb\n  # ## Create an external vs internal lb\n  # alb.ingress.kubernetes.io/scheme: /internet-facing|internal/\n  # ## Which security groups to attach. Omission will result in a cluster-managed, publicly-accessible SG being created and attached\n  # alb.ingress.kubernetes.io/security-groups: sg-723a380a,sg-a6181ede,sg-a5181edd\n  # ## Certificate to attach to https listener\n  # alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:511282955421:certificate/04793e46-aea2-4af8-8165-7e66827d12f2\n  # ## Always include at least these tags\n  # alb.ingress.kubernetes.io/tags: Environment={{ .Values.environment }},costcode={{ .Values.costcode }}\n  # ## Logs to S3, always include\n  # alb.ingress.kubernetes.io/attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=cgws-elb-logs\n  # ## Ports to create listeners on\n  # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":80,\"HTTPS\":443}]'\n  # ## Healthcheck config\n  # alb.ingress.kubernetes.io/healthcheck-interval-seconds: \"6\"\n  # alb.ingress.kubernetes.io/healthcheck-path: /\n  # alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n  # alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \"5\"\n  # alb.ingress.kubernetes.io/healthy-threshold-count: \"4\"\n  # alb.ingress.kubernetes.io/unhealthy-threshold-count: \"2\"\n  # alb.ingress.kubernetes.io/success-codes: \"200\"\n  # ## Deregistration is frustratingly long (5 or 10 minutes) by default, so set it to something less change prohibitive\n  # alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=60\n  # ## Create lb as ipv6+ipv4 or just ipv4\n  # alb.ingress.kubernetes.io/ip-address-type: /dualstack|ipv4/\n  # ## TLS security policy to use on HTTPS listeners. Try not to use anything older than ELBSecurityPolicy-TLS-1-2-2017-01\n  # alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-2017-01\n  # ## Redirects HTTP to HTTPS natively (within the LB)\n  # ## See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/tasks/ssl_redirect/\n  # ## NOTE: This has not been tested since the original implementation which was thoroughly broken\n  # alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\": {\"Protocol\": \"HTTPS\", \"StatusCode\": \"HTTP_301\"}}'\n\nservice:\n  ## Allows arbitrary service annotations like those for AWS load balancers to be added\n  ## See https://kubernetes.io/docs/concepts/services-networking/ for aws LoadBalancer annotations\n  annotations: {}\n    # external-dns.alpha.kubernetes.io/hostname: \"service.env.cgws.com.au\"\n    # external-dns.alpha.kubernetes.io/ttl: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:ap-southeast-2:\u003caccount-id\u003e:certificate/\u003ccert-id\u003e\"\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n    # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n    # service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \"environment=env,costcode=costcode\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \"\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \"3\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \"20\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \"5\"\n    # service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: \"sg-53fae93f,sg-42efd82e\"\n\n  servicePort: 9399  # Main port from which the app serves\n\n  type: ClusterIP  # (ClusterIP|LoadBalancer) Should almost always be ClusterIP\n  \n  clusterIP: \"\"  # Probably leave blank\n  loadBalancerIP: \"\"  # Set ONLY if the app must have its own LB, for general external access use ingress\n  loadBalancerSourceRanges: []  # Security group inbound source IP ranges with allowed access\n\n## nodeSelectors, tolerations, and affinity allow running the pods in specific places,\n## like on master nodes instead of workers.\n## Try not to use these.\n\n## Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n\n## List of node taints to tolerate (requires Kubernetes \u003e= 1.6)\ntolerations: []\n\n## Affinity\naffinity: []\n",
                            "vars.%": "6",
                            "vars.APP_NAME": "sql_exporter",
                            "vars.COSTCODE": "cotd",
                            "vars.ENV_NAME": "ops",
                            "vars.ENV_TYPE": "production",
                            "vars.IMAGE": "cgws/sql_exporter",
                            "vars.SCALING_ENABLED": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "local_file.values_yamls": {
                    "type": "local_file",
                    "depends_on": [
                        "data.template_file.values.*",
                        "local.app_name",
                        "local.environments"
                    ],
                    "primary": {
                        "id": "53ac3b3b8c18f1b62be473d9e9262ab2ec68ca29",
                        "attributes": {
                            "content": "---\ncostcode: cotd\n\nconfig:\n  # See veritas flux release for main config\n  \n  # Global settings and defaults.\n  global:\n    # Scrape timeouts ensure that:\n    #   (i)  scraping completes in reasonable time and\n    #   (ii) slow queries are canceled early when the database is already under heavy load\n    # Prometheus informs targets of its own scrape timeout (via the \"X-Prometheus-Scrape-Timeout-Seconds\" request header)\n    # so the actual timeout is computed as:\n    #   min(scrape_timeout, X-Prometheus-Scrape-Timeout-Seconds - scrape_timeout_offset)\n    #\n    # If scrape_timeout \u003c= 0, no timeout is set unless Prometheus provides one. The default is 10s.\n    scrape_timeout: 5s\n    # Subtracted from Prometheus' scrape_timeout to give us some headroom and prevent Prometheus from timing out first.\n    #\n    # Must be strictly positive. The default is 500ms.\n    scrape_timeout_offset: 500ms\n    # Minimum interval between collector runs: by default (0s) collectors are executed on every scrape.\n    min_interval: 10s\n    # Maximum number of open connections to any one target. Metric queries will run concurrently on multiple connections,\n    # as will concurrent scrapes.\n    #\n    # If max_connections \u003c= 0, then there is no limit on the number of open connections. The default is 3.\n    max_connections: 10\n    # Maximum number of idle connections to any one target. Unless you use very long collection intervals, this should\n    # always be the same as max_connections.\n    #\n    # If max_idle_connections \u003c= 0, no idle connections are retained. The default is 3.\n    max_idle_connections: 10\n\n## Details about the image to be pulled.\nimage:\n  name: 362995399210.dkr.ecr.ap-southeast-2.amazonaws.com/cgws/sql_exporter\n  tag: latest\n  # Try to leave as IfNotPresent, but Always can be used to force image updates at the cost of slower boot times\n  pullPolicy: IfNotPresent\n\n# priorityClassName: \"\"  # Not working yet, allows defining relative pod priority\n\n## Number of instances of the same service to run (how many copies of the pod to run)\nreplicas: 1\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxUnavailable: 1\n\nscaling:\n  enabled: 0\n  min: 1\n  max: 4\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        targetAverageUtilization: 90\n# awsenv required config for envvar loading\nssm:\n  awsRegion: ap-southeast-2\n  path: /sql_exporter/production/ops\n\n# vaultenv required config for envvar loading\n# vault:\n#   secret: /app/appname/environment\n#   aws_auth_mount_path: aws-rbt\n#   role: kubernetes\n\n## Arbitrary Annotations to be added to pods\n## Good for kube2iam, etc\npodAnnotations:\n  log.config.scalyr.com/attributes.parser: loggerNode\n\npodMetrics:\n  port: 9399\n  path: /metrics\n\n## Add any additional pod labels\npodLabels: {}\n\n## Add extra arguments to the container execution command\nextraArgs: {}\n#  key: value   # provided to app as --key=value\n#  key          # provided to app as --key\n\n## Environment variables for the container\n## The default chart deployment is for a single container and will require\n## modification for multiples.\n\nextraEnv: []\n\npodVolumes: false\n\n## These two health check probes should be used to ensure desired behaviour\n## during app failure, maintenance, or other issues. They should point at\n## endpoints that produce different results under different circumstances,\n## if possible.\n## Liveness probes destroy and replace the pod on failure\n# livenessProbe:\n#   httpGet:\n#     path: /healthz\n#     port: 9910\n#     initialDelaySeconds: 3  # Pod boot time\n#     scheme: HTTP\n## Readiness probes leave the pod running but detach from load balancers\n## during failure (i.e. the pods stop receiving requests)\n# readinessProbe:\n#   httpGet:\n#     path: /healthz?isDbUp\u0026isRedisUp\u0026amIConfigured\n#     port: 9910\n#     initialDelaySeconds: 3\n#     scheme: HTTPS\n\n## Every service requires a sane limit as kube doesn't manage resources well enough to protect its core functions\n## Ensure the limit is large enough not to get in the way - it should only be hit\n## if something has gone terribly wrong as it will result in the death of the pod \nresources:\n  limits:\n    memory: 2048Mi\n    cpu: 2000m\n  # Request about what the app will need on average, but low ball it\n  # Requests are **guarantees** and are bad for utilisation optimisation\n  requests:\n    memory: 50Mi\n    cpu: 10m\n\n## rbac should only be necessary for accessing the cluster.\n## Come see ops if you need it.\n## This is here mostly as a placeholder.\nrbac:\n  ## If true, create \u0026 use RBAC resources\n  ##\n  create: false\n  # Beginning with Kubernetes 1.8, the api is stable and v1 can be used.\n  apiVersion: v1\n\n  ## Ignored if rbac.create is true\n  ##\n  serviceAccountName: default\n\ningress:\n  enabled: false\n  labels: {}\n  # Used to create Ingress record (should used with service.type: ClusterIP).\n  # hosts:\n  #  - service.cgws.com.au\n  annotations: {}\n  ## External-DNS will create and manage the dns record for you\n  #   external-dns.alpha.kubernetes.io/hostname: \"service.cgws.com.au\"\n  #   external-dns.alpha.kubernetes.io/ttl: \"60\"\n  ## For apps that don't need dedicated load balancers (not many should), utilise the\n  ## existing cluster nginx ingresses: nginx|nginx-dmz|nginx-public\n  ## Depending on the cluster, nginx-public may not exist - talk to ops if you need it.\n  #   kubernetes.io/ingress.class: nginx-dmz\n  ## For apps that require a dedicated load balancer, an ALB can be configured as below.\n  ## Note that where these example values are regex they will not work without modification.\n  ## See https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/86ceee1e0ff05b0f414bf831f131a4ab95bcf923/docs/ingress-resources.md\n  # kubernetes.io/ingress.class: alb\n  # ## Create an external vs internal lb\n  # alb.ingress.kubernetes.io/scheme: /internet-facing|internal/\n  # ## Which security groups to attach. Omission will result in a cluster-managed, publicly-accessible SG being created and attached\n  # alb.ingress.kubernetes.io/security-groups: sg-723a380a,sg-a6181ede,sg-a5181edd\n  # ## Certificate to attach to https listener\n  # alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-southeast-2:511282955421:certificate/04793e46-aea2-4af8-8165-7e66827d12f2\n  # ## Always include at least these tags\n  # alb.ingress.kubernetes.io/tags: Environment={{ .Values.environment }},costcode={{ .Values.costcode }}\n  # ## Logs to S3, always include\n  # alb.ingress.kubernetes.io/attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=cgws-elb-logs\n  # ## Ports to create listeners on\n  # alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":80,\"HTTPS\":443}]'\n  # ## Healthcheck config\n  # alb.ingress.kubernetes.io/healthcheck-interval-seconds: \"6\"\n  # alb.ingress.kubernetes.io/healthcheck-path: /\n  # alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n  # alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \"5\"\n  # alb.ingress.kubernetes.io/healthy-threshold-count: \"4\"\n  # alb.ingress.kubernetes.io/unhealthy-threshold-count: \"2\"\n  # alb.ingress.kubernetes.io/success-codes: \"200\"\n  # ## Deregistration is frustratingly long (5 or 10 minutes) by default, so set it to something less change prohibitive\n  # alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=60\n  # ## Create lb as ipv6+ipv4 or just ipv4\n  # alb.ingress.kubernetes.io/ip-address-type: /dualstack|ipv4/\n  # ## TLS security policy to use on HTTPS listeners. Try not to use anything older than ELBSecurityPolicy-TLS-1-2-2017-01\n  # alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-2017-01\n  # ## Redirects HTTP to HTTPS natively (within the LB)\n  # ## See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/tasks/ssl_redirect/\n  # ## NOTE: This has not been tested since the original implementation which was thoroughly broken\n  # alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\": {\"Protocol\": \"HTTPS\", \"StatusCode\": \"HTTP_301\"}}'\n\nservice:\n  ## Allows arbitrary service annotations like those for AWS load balancers to be added\n  ## See https://kubernetes.io/docs/concepts/services-networking/ for aws LoadBalancer annotations\n  annotations: {}\n    # external-dns.alpha.kubernetes.io/hostname: \"service.env.cgws.com.au\"\n    # external-dns.alpha.kubernetes.io/ttl: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \"60\"\n    # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:ap-southeast-2:\u003caccount-id\u003e:certificate/\u003ccert-id\u003e\"\n    # service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https\n    # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n    # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n    # service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \"environment=env,costcode=costcode\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \"\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \"3\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \"20\"\n    # service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \"5\"\n    # service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: \"sg-53fae93f,sg-42efd82e\"\n\n  servicePort: 9399  # Main port from which the app serves\n\n  type: ClusterIP  # (ClusterIP|LoadBalancer) Should almost always be ClusterIP\n  \n  clusterIP: \"\"  # Probably leave blank\n  loadBalancerIP: \"\"  # Set ONLY if the app must have its own LB, for general external access use ingress\n  loadBalancerSourceRanges: []  # Security group inbound source IP ranges with allowed access\n\n## nodeSelectors, tolerations, and affinity allow running the pods in specific places,\n## like on master nodes instead of workers.\n## Try not to use these.\n\n## Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n\n## List of node taints to tolerate (requires Kubernetes \u003e= 1.6)\ntolerations: []\n\n## Affinity\naffinity: []\n",
                            "filename": "sql_exporter/values-ops.yaml",
                            "id": "53ac3b3b8c18f1b62be473d9e9262ab2ec68ca29"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.local"
                }
            },
            "depends_on": []
        }
    ]
}
